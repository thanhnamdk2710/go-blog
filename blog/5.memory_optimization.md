# Memory Optimization

Go is famous for its simplicity and built-in concurrency model - but performace is where Go truly shines. To build scalable backend systems, understanding how Go manages memory is essential.

In this article, we'll cover three advanced concepts every developer must master:

1. Escape Analysis - Why variables move to the heap
2. Allocation Optimization - How to reduce heap pressure
3. GC Tuning - How to optimize garbage collection for latency-sensitive services

Let's dive into practical explanations with real production examples.

## 1. Escape Analysis - How Go Decides Heap vs Stack

Go compiles variables into stack or heap memory.

- Stack allocation → very fast, automatically freed
- Heap allocation → requires the garbage collector (slower)

Escape analysis determines where a variable should live.

Run it via:

```bash
go build -gcflags="-m"
```

### Example 1: Returning a Pointer Causes Escape

```go
func newUser() *User {
    u := User{Name: "Nam"}
    return &u
}
```

Output:

```bash
&u escape to heap
```

Why? Because `u` must survive after the function returns → stored on heap

### Example 2: Avoiding Escape

```go
func process() {
    u := User{Name: "Nam"}
    fmt.Println(u.Name)
}
```

Stay on the stack → no GC overhead.

### Example 3: Interface Causes Escape

```go
func Print(v interface{}) {
    fmt.Println(v)
}
```

If you pass a value:

```bash
v escapes to heap
```

Reason: Go boxes data when converting to `interface{}` → heap allocation

### Example 4: Slice of pointers causes escape

```go
users := []*User{
    &User{Name: "A"},
    &User{Name: "B"},
}
```

Each `&User` escapes → heap.

Better:

```go
users := []User{{Name: "A"}, {Name: "B"}}
```

Stores values directly → no heap allocations.

### Tips

Avoid unnecessary pointers, interfaces, closures, and heap-based slices when possible.

Reducing heap allocations yields:

- fewer garbage collection
- lower latency
- higher throughput

## Allocation Optimization - Reduce GC Pressure

To optimize memory, reduce the number of objects allocated on the heap.

Let's look at patterns that often cause hidden allocations.

### 1. Avoid Allocating on Hot Paths

Bad:

```go
func handler() {
    buf := make([]byte, 0)
}
```

Better: reuse buffers with `sync.Pool`

```go
var bufPool = sync.Pool{
    New: func() interface{} {
        return make([]byte, 4096)
    },
}

func handler() {
    buf := bufPool.Get().([]byte)
    defer bufPool.Put(buf)
}
```

Production use cases:

- JSON encoding
- log encoding
- temporary working buffers
- compression, hashing

### 2. Preallocate Slices

Bad:

```go
arr := []int{}
for i := 0; i < 100000; i++ {
    arr = append(arr, i)
}
```

Repeated automatic reallocations + copies → expensive

Good:

```go
arr := make([]int, 0, 100000)
```

## 3. Avoid Large Struct Copies

Bad:

```go
func process(u User) {} // copies entire struct
```

Good:

```go
func process(u *User) {}
```

Rule of thumb:

- Use values for small structs
- Use pointer for large structs

### 4. Use `strings.Builder` for String concatenation

Bad (allocation-heavy):

```go
s := ""
for _, v := range list {
    s += v
}
```

Good:

```go
var b strings.Builder
b.Grow(1024)
for _, v := range list {
    b.WriteString(v)
}
return b.String()
```

## 3. Garbage Collector Tuning (GC) - Reduce Latency & CPU Usage

Go uses a concurrent mark-and-sweep garbage collector.

GC pauses are small, but high allocation programs can trigger GC too frequently, causing:

- Increased CPU usage
- Latency spikes
- Reduced throughput

You can tune Go's GC behavior.

### GOGC - Control GC Frequency

Environment variable:

```bash
export GOGC=100
```

Default = 100
Meaning: GC trigger when heap grows by 100%.

**Increase GOGC → fewer GCs**

`GOGC=200` (good for throughput, batch jobs)

**Decrease GOGC → more frequent GCs**

`GOGC=50` (good for low-latency)

### Profile Memory with pprof

```bash
go tool pprof --http=:8080 http://localhost:6060/debug/pprof/heap
```

Goals:

- Find high allocation functions
- Identify large/temporary objects
- Detect goroutine leaks
- Measure allocation rate (allocs/sec)

### Example - JSON Processing Service

Your service:

- Receives HTTP requests
- Parses 10,000 JSON documents/min
- Enriches & stores results in PostgreSQL

Initial problem:

- High allocation rate
- GC running every 50ms
- Latency spikes to 300ms

Fixes applied:

1. Replace `map[string]interface{}` with struct decoding

Before:

```go
var data map[string]interface{}
json.Unmarshal(body, &data)
```

After:

```go
var payload UserPayload
json.Unmarshal(body, &payload)
```

→ 80% fewer allocations.

2. Use `sync.Pool` for JSON encoder/decoder buffers

```go
var jsonPool = sync.Pool{
    New: func() interface{} {
        return make([]byte, 4096)
    },
}
```

→ Reduces heap pressure significantly.

3. Increase GOGC for throughput service

```bash
export GOGC=200
```

4. Preallocate slices for batch DB inserts

```go
items := make([]User, 0, batchSize)
```

Result:

- GC cycle dropped from every 50ms → 400ms
- Latency dropped from 300ms → <40ms
- CPU usage dropped by 30%

## Best Practice Summary

Escape Analysis

- Avoid unnecessary pointers
- Avoid interfaces on hot paths
- Avoid closures capturing variables
- Prefer structs over pointers in slices

Allocation Optimization

- Preallocate slices
- Reuse buffers using `sync.Pool`
- Use `strings.Builder` for concatenation
- Reduce temporary objects
- Use pointers for large structs

GC Tuning

- Use `GOGC` to adjust GC aggressiveness
- Profile allocations with pprof
- Optimize hot code paths
- Reduce heap allocations to reduce GC frequency

## Final Thoughts

Memory optimization is a fundamental skill for engineers. Mastering escape analysis, allocation strategies, and GC tuning enables you to build:

- Faster services
- Lower latency
- More stable microservices
- More efficient backend pipelines

Whether you're processing millions of Kafka messages, running a high-QPS API, or building a batch ETL service, these optimizations directly improve system performance.
