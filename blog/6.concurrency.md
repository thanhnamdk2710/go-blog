# Mastering Concurrency

Go's concurrency model - goroutines, channels, and synchronization primitives - allows developers to build high-performance and scalable backend systems. But writing correct and efficient concurrent code requires understanding the right patterns.

In this article, we explore six fundamental concurrency patterns used in Go systems:

- Fan-Out / Fan-In
- Pipeline Pattern
- Select Pattern
- Mutex
- RWMutex
- Atomic Operations

These patterns form the backbone of production systems such as Kafka consumers, ETL pipelines, HTTP workers, caching layers, metric collectors, and more.

## 1. Fan-Out / Fan-In

When to use

- Distribute workload to multiple workers
- Increase throughput
- Run tasks in parallel
- CPU-heavy or IO-heavy operations

"Fan-Out" starts multiple goroutines to perform work.
"Fan-In" merges their results back into one channel.

### Example: Fetch User Profiles from External APIs in Parallel

```go
func fetchUser(id int) string {
    time.Sleep(100 * time.Millisecond) // simulate API call
    return fmt.Sprintf("User %d", id)
}

func main() {
    ids := []int{1, 2, 3, 4, 5}

    jobs := make(chan int)
    results := make(chan string)

    // Fan-out: start 3 workers
    for w := 1; w <= 3; w++ {
        go func() {
            for id := range jobs {
                results <- fetchUser(id)
            }
        }()
    }

    // Send jobs
    go func() {
        for _, id := range ids {
            jobs <- id
        }
        close(jobs)
    }()

    // Fan-in: collect results
    for range ids {
        fmt.Println(<-results)
    }
}
```

Benefits

- Efficient parallelization
- Bounded concurrency
- Works well with worker pools

## 2. Pipeline Pattern

When to use

- Stream data through multiple processing stages
- Combine transformations
- Build modular ETL or event processing systems

A pipeline is a series of stages connected by channels.

```txt
Source → Stage A → Stage B → Stage C → Output
```

Each stage does one thing and passes results down the chain.

### Example: ETL Pipeline (Extract → Transform → Load)

```go
// Stage 1: extract
func extract(nums []int) <-chan int {
    out := make(chan int)
    go func() {
        for _, n := range nums {
            out <- n
        }
        close(out)
    }()
    return out
}

// Stage 2: transform
func square(in <-chan int) <-chan int {
    out := make(chan int)
    go func() {
        for n := range in {
            out <- n * n
        }
        close(out)
    }()
    return out
}

// Stage 3: load
func printAll(in <-chan int) {
    for v := range in {
        fmt.Println(v)
    }
}

func main() {
    nums := []int{1, 2, 3, 4, 5}
    pipeline := square(extract(nums))
    printAll(pipeline)
}
```

Why pipelines matter

- Decomposed, testable stages
- Easier to parallelize
- Natural backpressure handling

## 3. select Pattern

`select` lets a goroutine wait on multiple channel operations.

When to use

- Cancellation
- Timeouts
- Multiplexing channels
- Event streaming
- Worker shutdown

### Example: Handle Timeout on Slow API Call

```go
func slowAPI() <-chan string {
    ch := make(chan string)
    go func() {
        time.Sleep(2 * time.Second)
        ch <- "OK"
    }()
    return ch
}

func main() {
    select {
    case res := <-slowAPI():
        fmt.Println("Response:", res)
    case <-time.After(1 * time.Second):
        fmt.Println("Timeout")
    }
}
```

Use cases

- gRPC deadline enforcement
- Kafka consumer shutdown
- Streaming services
- Websocket cleanups

## 4. Mutex (Mutual Exclusion)

Used to protect shared mutable state from race conditions.

When to use

- Multiple goroutines modify shared data
- Performance is not highly read-heavy

### Example: Safe Counter with Mutex

```go
type Counter struct {
    mu sync.Mutex
    n  int
}

func (c *Counter) Inc() {
    c.mu.Lock()
    c.n++
    c.mu.Unlock()
}

func (c *Counter) Value() int {
    c.mu.Lock()
    defer c.mu.Unlock()
    return c.n
}
```

Why mutex works

- Prevents data races
- Simple and predictable

## 5. RWMutex (Read-Write Lock)

Allows multiple readers but only one writer.

When to use

- Many reads, few writes
- Caching
- Configuration data
- In-memory lookups

### Example: In-memory Cache

```go
type Cache struct {
    mu    sync.RWMutex
    store map[string]string
}

func (c *Cache) Get(key string) string {
    c.mu.RLock()
    defer c.mu.RUnlock()
    return c.store[key]
}

func (c *Cache) Set(key, value string) {
    c.mu.Lock()
    c.store[key] = value
    c.mu.Unlock()
}
```

Benefits

- Better read performance
- Avoids unnecessary contention

## 6. Atomic Operations

Atomic operations provide lock-free, extremely fast operations.

When to use

- High-frequency counters
- Metrics
- Reference swapping
- Fast ID generation
- Non-blocking algorithms

### Example: Atomic Counter

```go
var count atomic.Int64

func worker() {
    for i := 0; i < 1000; i++ {
        count.Add(1)
    }
}
```

Advantages

- No mutex overhead
- Very fast for simple operations

Limitations

- Only works for 64-bit aligned primitives
- Not suitable for complex critical sections

## Backend Example

Use Case: Log Processing Service

- Kafka consumer reads events
- Worker pool handles transformation
- Pipeline validates → normalizes → stores
- Select pattern handles timeouts
- RWMutex protects in-memory lookup table
- Atomic counters track metrics

Architecture Overview

```txt
Kafka → Fan-Out Workers → Pipeline (validate → enrich → save)
             ↓
         Select for timeout
             ↓
    Cache + RWMutex + Atomic metrics
```

This combination is extremely common in:

- Log ingestion platforms
- ETL services
- Event-driven microservices
- Billing pipelines
- Real-time analytics systems

## Final Thoughts

These concurrency patterns form the foundation of high-performance systems. Mastering them allows you to build:

- Faster microservices
- Scalable ETL pipeline
- Reliable worker pools
- Low-latency APIs
- Efficient streaming systems

Concurrency is where Go shines strongest. Learn these patterns well, and your backend architecture will level up significantly.
